{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T07:59:19.754937Z",
     "start_time": "2024-04-03T07:57:33.914675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (2.16.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (69.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aywski\\appdata\\roaming\\python\\python312\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31657880dcff5207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T07:59:35.061723Z",
     "start_time": "2024-04-03T07:59:35.050434Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_dataset(page_dataset, for_inference):\n",
    "    labeled_text_dataset = []\n",
    "    for page in page_dataset:\n",
    "        page_words = page[\"representativeData\"][\"page_data_words\"]\n",
    "        \n",
    "        geo_dictionary = {}\n",
    "        if not for_inference:\n",
    "            page_answers = page.get(\"answers\")\n",
    "            for page_answer in page_answers[0][\"answer\"]:\n",
    "                geo_label = page_answer[\"id\"]\n",
    "                for geo_part in page_answer[\"data\"]:\n",
    "                    for index in range(geo_part[\"start\"], geo_part[\"end\"]):\n",
    "                        geo_dictionary[index] = geo_label\n",
    "        \n",
    "        labeled_text = []\n",
    "        for word_index, word in enumerate(page_words):\n",
    "            word_label = \"0\" if for_inference else geo_dictionary.get(word_index, \"O\")\n",
    "            labeled_text.append((word, word_label))\n",
    "        \n",
    "        if not for_inference:\n",
    "            labeled_text_dataset.append(labeled_text)\n",
    "        else:\n",
    "            labeled_text_dataset.append((page[\"taskId\"], labeled_text))\n",
    "    \n",
    "    return labeled_text_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7663d231d4b68c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T07:59:35.791794Z",
     "start_time": "2024-04-03T07:59:35.785361Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_labeled_dataset(dataset_path, for_inference=False):\n",
    "    with open(dataset_path, encoding=\"utf-8\") as json_dataset:\n",
    "        dataset = json.load(json_dataset)\n",
    "        \n",
    "    labeled_dataset = transform_dataset(dataset[\"data\"][\"results\"], for_inference)\n",
    "    return labeled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7cbd9ffebc2c688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T07:59:41.821943Z",
     "start_time": "2024-04-03T07:59:41.803947Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_validation_result(X_validation, y_pred):\n",
    "    validation_result = []\n",
    "    \n",
    "    for ((task_id, _), predictions) in zip(X_validation, y_pred):\n",
    "        answers = {}\n",
    "        current_label = None\n",
    "        start_index = None\n",
    "        \n",
    "        for current_index, label in enumerate(predictions):\n",
    "            if label == current_label:\n",
    "                continue\n",
    "            else:\n",
    "                if current_label is not None and current_label != \"O\":\n",
    "                    if current_label not in answers:\n",
    "                        answers[current_label] = []\n",
    "                    answers[current_label].append({\"start\": start_index, \"end\": current_index})\n",
    "                \n",
    "                if label != \"0\":\n",
    "                    current_label = label\n",
    "                    start_index = current_index\n",
    "                else:\n",
    "                    current_label = None\n",
    "    \n",
    "        if current_label is not None and current_label != \"O\":\n",
    "            if current_label not in answers:\n",
    "                answers[current_label] = []\n",
    "            answers[current_label].append({\"start\": start_index, \"end\": len(predictions)})\n",
    "        \n",
    "        validation_answers = []\n",
    "        for label, segments in answers.items():\n",
    "            validation_answers.append({\"id\": label, \"data\": segments})\n",
    "        \n",
    "        validation_result.append({\n",
    "            \"taskId\": task_id,\n",
    "            \"answer\": validation_answers\n",
    "        })\n",
    "        \n",
    "    return validation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ffa052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def focal_loss(alpha=0.25, gamma=2.):\n",
    "    def focal_loss_parametrized(y_true, y_pred):\n",
    "        e = 1.e-9\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "        \n",
    "        model_output = tf.add(y_pred, e)\n",
    "        ce = tf.multiply(y_true, -tf.math.log(model_output))\n",
    "        w = tf.multiply(y_true, tf.pow(tf.subtract(1., model_output), gamma))\n",
    "        fl = tf.multiply(alpha, tf.multiply(w, ce))\n",
    "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "        return tf.reduce_mean(reduced_fl)\n",
    "    \n",
    "    return focal_loss_parametrized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ebb486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_labeled_dataset(\"../jsons/train_geo_extractor.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48399587",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_length = max([len(text) for text in train_dataset])\n",
    "\n",
    "words = [word for text in train_dataset for word, _ in text]\n",
    "words.append(\"UNKNOWN\")\n",
    "words.append(\"ENDPAD\")\n",
    "words = list(set(words))\n",
    "\n",
    "labels = list(set([label for text in train_dataset for _, label in text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c68c9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {word: index for index, word in enumerate(words)}\n",
    "label2index = {label: index for index, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cb9f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = [[word2index[word] for word, _ in text] for text in train_dataset]\n",
    "X_train = pad_sequences(maxlen=max_text_length, sequences=X_train, padding=\"post\", value=len(words) - 1)\n",
    "\n",
    "y_train = [[label2index[label] for _, label in text] for text in train_dataset]\n",
    "y_train = pad_sequences(maxlen=max_text_length, sequences=y_train, padding=\"post\", value=label2index[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1cd8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = [to_categorical(index, num_classes=len(labels)) for index in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96894246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m  2/175\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46:29\u001b[0m 37s/step - accuracy: 0.2490 - loss: 0.2186"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from uuid import uuid4\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Dense, Bidirectional, LSTM\n",
    "\n",
    "# Your model architecture\n",
    "model_input = Input(shape=(max_text_length,))\n",
    "embedding_output = Embedding(input_dim=len(words), output_dim=max_text_length, input_length=max_text_length)(model_input)\n",
    "dropout_output = Dropout(0.1)(embedding_output)\n",
    "lstm_output = Bidirectional(LSTM(units=300, return_sequences=True))(dropout_output)\n",
    "model_output = TimeDistributed(Dense(len(labels), activation=\"softmax\"))(lstm_output)\n",
    "model = Model(model_input, model_output)\n",
    "\n",
    "# Compilation\n",
    "model.compile(optimizer=\"adam\", loss=focal_loss(), metrics=[\"accuracy\"])\n",
    "\n",
    "# Training\n",
    "model.fit(X_train, np.array(y_train), batch_size=16, epochs=5)\n",
    "\n",
    "# Save the model\n",
    "random_model_name = str(uuid4())\n",
    "model.save(f\"saved_model/{random_model_name}\")\n",
    "print(f\"Model {random_model_name} has successfully been saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a45e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy scikit-learn gensim tf2crf tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import FastText\n",
    "\n",
    "sentences = [[word for word, _ in text] for text in train_dataset]\n",
    "\n",
    "model = FastText(sentences, vector_size=100, window=3, min_count=1, workers=os.cpu_count(), sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ffa22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2index), 100))\n",
    "\n",
    "for word, index in word2index.items():\n",
    "    embedding_vector = model.wv[word]\n",
    "    embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim=len(word2index),\n",
    "                           output_dim=100,\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=max_text_length,\n",
    "                           trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "from tf2crf import CRF, ModelWithCRFLoss\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, TimeDistributed\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Dense\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Attention\n",
    "\n",
    "model_input = Input(shape=(max_text_length, ))\n",
    "model = Embedding(input_dim=len(words), output_dim=max_text_length, input_length=max_text_length)(model_input)\n",
    "model = Dropout(0.1)(model)\n",
    "\n",
    "lstm_output = Bidirectional(LSTM(units=100, return_sequences=True))(model)\n",
    "\n",
    "crf = CRF(dtype=\"float32\")\n",
    "model_output = crf(lstm_output)\n",
    "\n",
    "hybrid_model = Model(model_input, model_output)\n",
    "model = ModelWithCRFLoss(hybrid_model)\n",
    "model.compile(optimizer=\"adam\")\n",
    "\n",
    "model.fit(X_train, np.array(y_train), batch_size=8, epochs=5, use_multiprocessing=True, workers=os.cpu_count())\n",
    "\n",
    "random_model_name = str(uuid4())\n",
    "model.save(f\"saved_model/{random_model_name}\")\n",
    "print(f\"Model {random_model_name} has successfully been saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_labeled_dataset(\"datasets/test_geo_extractor.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc24f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model_name = input(\"Enter the model name: \")\n",
    "model_path = \"saved_model/\" + model_name\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"The model {model_name} does not exist!\")\n",
    "\n",
    "recognizer = models.load_model(model_path, custom_objects={\"focal_loss_parametrized\": focal_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8459387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_test = [[word2index.get(word, word2index[\"UNKNOWN\"]) for word, _ in text] for text in test_dataset]\n",
    "X_test = pad_sequences(maxlen=max_text_length, sequences=X_test, padding=\"post\", value=len(word2index) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = recognizer.predict(X_test, use_multiprocessing=True, workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e043a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pad_pred_test = [[labels[np.argmax(prediction)] for prediction in text_prediction]\n",
    "                   for text_prediction in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37735b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = []\n",
    "\n",
    "for i, text in enumerate(test_dataset):\n",
    "    text_predictions = []\n",
    "    for j, (word, _) in enumerate(text):\n",
    "        if j < len(y_pad_pred_test[i]):\n",
    "            text_predictions.append((word, y_pad_pred_test[i][j]))\n",
    "            \n",
    "    y_pred_test.append(text_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cad377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "y_test_flat = [label for text in test_dataset for _, label in text]\n",
    "y_pred_flat = [label for text in y_pred_test for _, label in text]\n",
    "\n",
    "print(classification_report(y_test_flat, y_pred_flat))\n",
    "print(f\"Matthews Correlation Coefficient: {matthews_corrcoef(y_test_flat, y_pred_flat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = get_labeled_dataset(\"datasets/val_no_answer_geo_extractor.json\", for_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0121830",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = [[word2index.get(word, word2index[\"UNKNOWN\"]) for word, _ in text]\n",
    "                for task_id, text in validation_dataset]\n",
    "X_validation = pad_sequences(maxlen=max_text_length, sequences=X_validation, padding=\"post\",\n",
    "                             value=len(word2index) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b510862",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_validation = recognizer.predict(X_validation)\n",
    "\n",
    "X_validation = [(task_id, text) for task_id, text in validation_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pad_pred_validation = [[labels[np.argmax(prediction)] for prediction in text_prediction]\n",
    "              for text_prediction in y_pred_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc56ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_validation = []\n",
    "\n",
    "for i, text in enumerate(validation_dataset):\n",
    "    text_predictions = []\n",
    "    for j, (word, _) in enumerate(text[1]):\n",
    "        if j < len(y_pad_pred_validation[i]):\n",
    "            text_predictions.append((word, y_pad_pred_validation[i][j]))\n",
    "            \n",
    "    y_pred_validation.append(text_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "y_pred_validation = [[label for _, label in text] for text in y_pred_validation]\n",
    "\n",
    "validation_result = get_validation_result(X_validation, y_pred_validation)\n",
    "\n",
    "with open(\"lstm_validation_result.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(validation_result, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Validation result has been saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdf202",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scikit-learn torch datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12985d2a1bf00cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T07:59:58.477374Z",
     "start_time": "2024-04-03T07:59:52.416242Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"xlm-roberta-large-finetuned-conll03-english\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=10, ignore_mismatched_sizes=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"central_city\", \"geo_address\", \"geo_building\", \"geo_city\",\n",
    "              \"geo_district\", \"geo_microdistrict\", \"geo_region\",\n",
    "              \"geo_region_oblast\", \"geo_street\"]\n",
    "\n",
    "label_dictionary = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a57888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(dataset):\n",
    "    texts = [\" \".join([word for word, label in text]) for text in dataset]\n",
    "    tokenized_inputs = tokenizer(texts, padding=True, truncation=True, \n",
    "                                 is_split_into_words=False, return_tensors=\"pt\")\n",
    "    \n",
    "    encoded_labels = []\n",
    "    for i, text in enumerate(dataset):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(0)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_dictionary[text[word_idx][1]])\n",
    "            else:\n",
    "                label_ids.append(0)\n",
    "            previous_word_idx = word_idx\n",
    "        encoded_labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = encoded_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenized_train_dataset = tokenize_and_align_labels(train_dataset)\n",
    "tokenized_test_dataset = tokenize_and_align_labels(test_dataset)\n",
    "\n",
    "transformed_train_dataset = Dataset.from_dict(tokenized_train_dataset)\n",
    "transformed_test_dataset = Dataset.from_dict(tokenized_test_dataset)\n",
    "print(transformed_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d63438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != 0]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != 0]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0717cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=transformed_train_dataset,\n",
    "    eval_dataset=transformed_test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f383ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e177a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions_logits = predictions.predictions\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "predicted_labels = np.argmax(predictions_logits, axis=2)\n",
    "\n",
    "true_labels = [[label for label in sentence if label != -100] for sentence in true_labels]\n",
    "predicted_labels = [\n",
    "    [p for (p, label) in zip(prediction, labels) if label != -100]\n",
    "    for prediction, labels in zip(predicted_labels, true_labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = {id: label for label, id in label_to_id.items()}\n",
    "\n",
    "def labels_to_names(labels, id_to_label):\n",
    "    return [[id_to_label[label] for label in sentence] for sentence in labels]\n",
    "\n",
    "predicted_label_names = labels_to_names(predicted_labels, id_to_label)\n",
    "true_label_names = labels_to_names(true_labels, id_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import itertools\n",
    "\n",
    "true_labels_flat = list(itertools.chain(*true_label_names))\n",
    "predicted_labels_flat = list(itertools.chain(*predicted_label_names))\n",
    "\n",
    "print(classification_report(true_labels_flat, predicted_labels_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e1b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas tqdm scikit-learn tensorflow transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e629147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_labeled_dataset(\"datasets/train_geo_extractor.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "label_list = [\"O\", \"central_city\", \"geo_address\", \"geo_building\", \"geo_city\",\n",
    "              \"geo_district\", \"geo_microdistrict\", \"geo_region\",\n",
    "              \"geo_region_oblast\", \"geo_street\"]\n",
    "\n",
    "label_dictionary = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "for text in train_dataset:\n",
    "    sentences.append([word for word, _ in text])\n",
    "    labels.append([label for _, label in text])\n",
    "\n",
    "encoded_labels = []\n",
    "\n",
    "for labels_for_sentence in labels:\n",
    "    encoded_labels.append([label_dictionary.get(label) for label in labels_for_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4404be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "max_sentence_length = max([len(text) for text in train_dataset])\n",
    "\n",
    "def tokenize(data, max_len):\n",
    "    input_ids = list()\n",
    "    attention_mask = list()\n",
    "    \n",
    "    for index in tqdm(range(len(data))):\n",
    "        encoded_data = tokenizer.encode_plus(data[index],\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=max_len,\n",
    "                                            is_split_into_words=True,\n",
    "                                            return_attention_mask=True,\n",
    "                                            padding=\"max_length\",\n",
    "                                            truncation=True,\n",
    "                                            return_tensors=\"np\")\n",
    "        \n",
    "        input_ids.append(encoded_data[\"input_ids\"])\n",
    "        attention_mask.append(encoded_data[\"attention_mask\"])\n",
    "        \n",
    "    return np.vstack(input_ids), np.vstack(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, encoded_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "input_ids, attention_mask = tokenize(X_train, max_len=max_sentence_length)\n",
    "val_input_ids, val_attention_mask = tokenize(X_test, max_len=max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_labels(input_labels, max_len):\n",
    "    padded_labels = list()\n",
    "\n",
    "    for index in range(len(input_labels)):\n",
    "        padded_labels.append(np.array(input_labels[index] + [0] * (max_len - len(input_labels[index]))))\n",
    "    \n",
    "    return padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pad_labels(y_train, max_sentence_length)\n",
    "test_labels = pad_labels(y_test, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e386ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_labels = [to_categorical(index, num_classes=len(label_list)) for index in train_labels]\n",
    "test_labels = [to_categorical(index, num_classes=len(label_list)) for index in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d58a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "\n",
    "def instantiate_model(bert_model, max_len):\n",
    "    input_ids = Input(shape=(max_len, ), dtype=\"int32\")\n",
    "    attention_mask = Input(shape=(max_len, ), dtype=\"int32\")\n",
    "    bert_layer = bert_model(input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "    \n",
    "    embedding_layer = Dropout(0.3)(bert_layer[\"last_hidden_state\"])\n",
    "    output_layer = Dense(len(label_list), activation=\"softmax\")(embedding_layer)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=[output_layer])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.00001), loss=SparseCategoricalCrossentropy(), metrics=[Accuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model = instantiate_model(bert_model, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping_callback = EarlyStopping(mode=\"min\", patience=5)\n",
    "\n",
    "bert_history = model.fit([input_ids, attention_mask], np.array(train_labels),\n",
    "                        validation_data=([val_input_ids, val_attention_mask], np.array(test_labels)),\n",
    "                        epochs=25, batch_size=32,\n",
    "                        callbacks=early_stopping_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8ea3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
